{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11604603,"sourceType":"datasetVersion","datasetId":7278525},{"sourceId":12097002,"sourceType":"datasetVersion","datasetId":7615490},{"sourceId":12097846,"sourceType":"datasetVersion","datasetId":7616081}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torchvision\nprint(torch.__version__)\nprint(torchvision.__version__)\n\nprint(\"CUDA verf√ºgbar:\", torch.cuda.is_available())\nprint(\"Torch CUDA Version:\", torch.version.cuda)\n\ntorch.cuda.empty_cache()\nprint(torch.cuda.memory_summary())","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T12:45:56.681200Z","iopub.execute_input":"2025-06-08T12:45:56.681605Z","iopub.status.idle":"2025-06-08T12:45:56.687765Z","shell.execute_reply.started":"2025-06-08T12:45:56.681579Z","shell.execute_reply":"2025-06-08T12:45:56.686981Z"}},"outputs":[{"name":"stdout","text":"2.5.1+cu124\n0.20.1+cu124\nCUDA verf√ºgbar: True\nTorch CUDA Version: 12.4\n|===========================================================================|\n|                  PyTorch CUDA memory summary, device ID 0                 |\n|---------------------------------------------------------------------------|\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n|===========================================================================|\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n|---------------------------------------------------------------------------|\n| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n|---------------------------------------------------------------------------|\n| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n|---------------------------------------------------------------------------|\n| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n|---------------------------------------------------------------------------|\n| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n|---------------------------------------------------------------------------|\n| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n|---------------------------------------------------------------------------|\n| Allocations           |       0    |       0    |       0    |       0    |\n|       from large pool |       0    |       0    |       0    |       0    |\n|       from small pool |       0    |       0    |       0    |       0    |\n|---------------------------------------------------------------------------|\n| Active allocs         |       0    |       0    |       0    |       0    |\n|       from large pool |       0    |       0    |       0    |       0    |\n|       from small pool |       0    |       0    |       0    |       0    |\n|---------------------------------------------------------------------------|\n| GPU reserved segments |       0    |       0    |       0    |       0    |\n|       from large pool |       0    |       0    |       0    |       0    |\n|       from small pool |       0    |       0    |       0    |       0    |\n|---------------------------------------------------------------------------|\n| Non-releasable allocs |       0    |       0    |       0    |       0    |\n|       from large pool |       0    |       0    |       0    |       0    |\n|       from small pool |       0    |       0    |       0    |       0    |\n|---------------------------------------------------------------------------|\n| Oversize allocations  |       0    |       0    |       0    |       0    |\n|---------------------------------------------------------------------------|\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\n|===========================================================================|\n\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"#!pip install pip==23.3\n#!pip install -r /kaggle/input/piper-orig/piper-master/src/python/requirements.txt --force-reinstall\n#!pip cache purge\n\n#!pip uninstall --yes torchmetrics pytorch-lightning torch cython piper-phonemize  librosa numpy onnxruntime onnx onnxruntime-gpu matplotlib\n\n!pip install -q cython>=0.29.0 piper-phonemize==1.1.0 librosa>=0.9.2 numpy==1.26 onnxruntime>=1.15.0 pytorch-lightning\n!pip install --upgrade transformers\n5\n#!pip install torch==2.0.1 torchvision==0.15.2 --upgrade --force-reinstall","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T12:45:56.692611Z","iopub.execute_input":"2025-06-08T12:45:56.692824Z","iopub.status.idle":"2025-06-08T12:46:03.077220Z","shell.execute_reply.started":"2025-06-08T12:45:56.692803Z","shell.execute_reply":"2025-06-08T12:46:03.076283Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n","output_type":"stream"},{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"5"},"metadata":{}}],"execution_count":50},{"cell_type":"code","source":"if not os.path.exists(\"/kaggle/input/pretrained\"):\n    raise Exception(\"upload pretrained model as pretrained.zip first. Models can be downloaded from: https://huggingface.co/rhasspy/piper-voices/tree/v1.0.0\")\n!cp -r /kaggle/input/pretrained /kaggle/working/pretrained","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T12:46:03.078735Z","iopub.execute_input":"2025-06-08T12:46:03.078970Z","iopub.status.idle":"2025-06-08T12:46:04.929903Z","shell.execute_reply.started":"2025-06-08T12:46:03.078947Z","shell.execute_reply":"2025-06-08T12:46:04.929112Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"#@markdown # <font color=\"ffc800\"> **Install software.** üì¶\n#@markdown ---\n#@markdown ####In this cell the synthesizer and its necessary dependencies to execute the training will be installed. (this may take a while)\n\n\n!cd /kaggle/working/\n#!rm -r /kaggle/working/piper\n# clone:\n#!pip install -q -r requirements.txt\n#!git clone -q https://github.com/rmcpantoja/piper\n!git clone -q https://github.com/t-meinhardt/piper-master /kaggle/working/piper\n\n!ls /kaggle/working/\n\n!bash /kaggle/working/piper/src/python/build_monotonic_align.sh\n\n!wget -q \"https://raw.githubusercontent.com/coqui-ai/TTS/dev/TTS/bin/resample.py\"\n\n# Useful vars:\nuse_whisper = True\nprint(\"Done!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T12:46:04.930891Z","iopub.execute_input":"2025-06-08T12:46:04.931170Z","iopub.status.idle":"2025-06-08T12:46:11.764770Z","shell.execute_reply.started":"2025-06-08T12:46:04.931139Z","shell.execute_reply":"2025-06-08T12:46:11.763826Z"}},"outputs":[{"name":"stdout","text":"fatal: destination path '/kaggle/working/piper' already exists and is not an empty directory.\n'=0.29.0'   audio_cache   piper_output\t resample.py\n'=0.9.2'    packages\t  pretrained\t voice-de_DE-epic-medium\n'=1.15.0'   piper\t  python\nCompiling /kaggle/working/piper/src/python/piper_train/vits/monotonic_align/core.pyx because it changed.\n[1/1] Cythonizing /kaggle/working/piper/src/python/piper_train/vits/monotonic_align/core.pyx\n/usr/local/lib/python3.11/dist-packages/Cython/Compiler/Main.py:381: FutureWarning: Cython directive 'language_level' not set, using '3str' for now (Py3). This has changed from earlier releases! File: /kaggle/working/piper/src/python/piper_train/vits/monotonic_align/core.pyx\n  tree = Parsing.p_module(s, pxd, full_module_name)\nperformance hint: core.pyx:7:5: Exception check on 'maximum_path_each' will always require the GIL to be acquired.\nPossible solutions:\n\t1. Declare 'maximum_path_each' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n\t2. Use an 'int' return type on 'maximum_path_each' to allow an error code to be returned.\nperformance hint: core.pyx:38:6: Exception check on 'maximum_path_c' will always require the GIL to be acquired.\nPossible solutions:\n\t1. Declare 'maximum_path_c' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n\t2. Use an 'int' return type on 'maximum_path_c' to allow an error code to be returned.\nperformance hint: core.pyx:42:21: Exception check after calling 'maximum_path_each' will always require the GIL to be acquired.\nPossible solutions:\n\t1. Declare 'maximum_path_each' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n\t2. Use an 'int' return type on 'maximum_path_each' to allow an error code to be returned.\nDone!\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"# 1. Load existing dataset üì•\n#preparation:\n\n#@markdown ####**Important: the transcription means writing what the character says in each of the audios, and it must have the following structure:**\n\n#@markdown ##### For a single-speaker dataset:\n#@markdown * wavs/1.wav|This is what my character says in audio 1.\n#@markdown * wavs/2.wav|This, the text that the character says in audio 2.\n#@markdown * ...\n\n#@markdown ##### For a multi-speaker dataset:\n\n#@markdown * wavs/speaker1audio1.wav|speaker1|This is what the first speaker says.\n#@markdown * wavs/speaker1audio2.wav|speaker1|This is another audio of the first speaker.\n#@markdown * wavs/speaker2audio1.wav|speaker2|This is what the second speaker says in the first audio.\n#@markdown * wavs/speaker2audio2.wav|speaker2|This is another audio of the second speaker.\n#@markdown * ...\n\n#@markdown And so on. In addition, the transcript must be in a **.csv format. (UTF-8 without BOM). Save it as metadata.csv outside of folder \"wavs\"**\n\nimport os\nimport wave\nimport datetime\n\ndef get_dataset_duration(wav_path):\n    totalduration = 0\n    for file_name in [x for x in os.listdir(wav_path) if os.path.isfile(os.path.join(wav_path, x)) and \".wav\" in x]:\n        with wave.open(os.path.join(wav_path, file_name), \"rb\") as wave_file:\n            frames = wave_file.getnframes()\n            rate = wave_file.getframerate()\n            duration = frames / float(rate)\n            totalduration += duration\n    wav_count = len([x for x in os.listdir(wav_path) if x.endswith('.wav')])\n    duration_str = str(datetime.timedelta(seconds=round(totalduration, 0)))\n    return wav_count, duration_str\n\nsample_path = '/kaggle/input/samples/WAVs'\nwavs_dir = os.path.join(sample_path, \"wavs\")  \n\n\nif not os.path.exists(wavs_dir):\n    raise Exception(f\"WAV directory not found at: {wavs_dir}. Upload WAV files as samples.zip first.\")\n\naudio_count, dataset_dur = get_dataset_duration(wavs_dir)\nprint(f\"Opened dataset with {audio_count} wavs with duration {dataset_dur}.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T13:23:37.490776Z","iopub.execute_input":"2025-06-08T13:23:37.491314Z","iopub.status.idle":"2025-06-08T13:23:37.802900Z","shell.execute_reply.started":"2025-06-08T13:23:37.491287Z","shell.execute_reply":"2025-06-08T13:23:37.802145Z"}},"outputs":[{"name":"stdout","text":"Opened dataset with 23 wavs with duration 0:08:59.\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"print(\"Adapt parameters here\")\nlanguage = \"Deutsch\" \nmodel_name = \"epic_german\" \ndataset_format = \"ljspeech\"  \nsingle_speaker = True\nforce_sp = \" --single-speaker\" if single_speaker else \"\"\nsample_rate = \"16000\"\n\ncheckpoint_name_pretrained_model = \"epoch=6069-step=230660.ckpt\"\n\n# === Training Configuration ===\n\n# === Batch size ===\n# Number of training examples per step.\n# Higher = faster learning, but uses more memory.\nbatch_size = 4  # e.g., 16\n\n# === Floating-point precision ===\n# 16-mixed = mixed precision (faster, uses less RAM), 32 = full precision.\n# Recommended: 16-mixed for GPUs.\nprecision = \"16-mixed\"\n\n# === Model quality ===\n# Affects model size, training time, and audio resolution.\n# Options:\n# - x-low: 16kHz audio, 5‚Äì7M parameters\n# - medium: 22.05kHz audio, ~15‚Äì20M parameters\n# - high: 22.05kHz audio, 28‚Äì32M parameters\nquality = \"medium\"\n\n# === Checkpoint saving interval (in epochs) ===\n# How often to save a model checkpoint during training.\n# 1 = save after every epoch.\ncheckpoint_epochs = 1\n\n# === Number of best model checkpoints to keep ===\n# Keeps the most recent `num_ckpt` best models based on validation.\n# Set to 0 to disable saving multiple checkpoints.\nnum_ckpt = 2\n\n# === Save the final model ===\n# Save a \"last.ckpt\" file at the end of training.\n# If num_ckpt = 0, this will be the only saved model.\n# ‚ö†Ô∏è Not recommended for very small datasets (may result in a corrupt checkpoint).\nsave_last = True\n\n# === Logging interval ===\n# How often to log training progress (loss, metrics).\n# Lower = more detailed logs.\nlog_every_n_steps = 1000\n\n# === Max training epochs ===\n# Total number of full passes through the training dataset.\n# Use fewer epochs for small datasets.\nmax_epochs = 10\n\n# === Finetuning ===\n# Optional: if you want to continue training from a pre-trained model,\n# uncomment and set the path to the checkpoint file (.ckpt).\n# ft_command = \"--finetune path/to/model.ckpt\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T13:30:48.614334Z","iopub.execute_input":"2025-06-08T13:30:48.615152Z","iopub.status.idle":"2025-06-08T13:30:48.620772Z","shell.execute_reply.started":"2025-06-08T13:30:48.615125Z","shell.execute_reply":"2025-06-08T13:30:48.619952Z"}},"outputs":[{"name":"stdout","text":"Adapt parameters here\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"\nimport os\n\nuse_whisper = False\nif use_whisper:\n    import torch\n    from faster_whisper import WhisperModel\n    from tqdm import tqdm\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Using device: {device}\")\n\n    def make_dataset(path, language):\n        metadata = \"\"\n        text = \"\"\n        files = [f for f in os.listdir(path) if f.endswith(\".wav\")]\n        assert len(files) > 0, \"No WAV files found. Please check your path.\"\n        \n        metadata_file = open(f\"{path}/../metadata.csv\", \"w\", encoding='utf-8')\n        whisper = WhisperModel(\"large-v3\", device=device, compute_type=\"float16\")\n        for audio_file in tqdm(files):\n            full_path = os.path.join(path, audio_file)\n            segments, _ = whisper.transcribe(full_path, word_timestamps=False, language=language)\n            for segment in segments:\n                text += segment.text\n            text = text.strip().replace('\\n', ' ')\n            metadata_line = f\"{audio_file}|{text}\\n\"\n            metadata_file.write(metadata_line)\n            text = \"\"\n        metadata_file.close()\n        del whisper\n        print(\"Transcription and metadata creation done.\")\n\n\nlanguages = {\n    \"ÿ£ŸÑÿπŸéÿ±Ÿéÿ®ŸêŸä\": \"ar\",\n    \"Catal√†\": \"ca\",\n    \"ƒçe≈°tina\": \"cs\",\n    \"Dansk\": \"da\",\n    \"Deutsch\": \"de\",\n    \"ŒïŒªŒªŒ∑ŒΩŒπŒ∫Œ¨\": \"el\",\n    \"English (British)\": \"en\",\n    \"English (U.S.)\": \"en-us\",\n    \"Espa√±ol (Castellano)\": \"es\",\n    \"Espa√±ol (Latinoamericano)\": \"es-419\",\n    \"Suomi\": \"fi\",\n    \"Fran√ßais\": \"fr\",\n    \"Magyar\": \"hu\",\n    \"Icelandic\": \"is\",\n    \"Italiano\": \"it\",\n    \"·É•·Éê·É†·Éó·É£·Éö·Éò\": \"ka\",\n    \"“õ–∞–∑–∞“õ—à–∞\": \"kk\",\n    \"L√´tzebuergesch\": \"lb\",\n    \"‡§®‡•á‡§™‡§æ‡§≤‡•Ä\": \"ne\",\n    \"Nederlands\": \"nl\",\n    \"Norsk\": \"nb\",\n    \"Polski\": \"pl\",\n    \"Portugu√™s (Brasil)\": \"pt-br\",\n    \"Portugu√™s (Portugal)\": \"pt-pt\",\n    \"Rom√¢nƒÉ\": \"ro\",\n    \"–†—É—Å—Å–∫–∏–π\": \"ru\",\n    \"–°—Ä–ø—Å–∫–∏\": \"sr\",\n    \"Svenska\": \"sv\",\n    \"Kiswahili\": \"sw\",\n    \"T√ºrk√ße\": \"tr\",\n    \"—É–∫—Ä–∞—óÃÅ–Ω—Å—å–∫–∞\": \"uk\",\n    \"Ti·∫øng Vi·ªát\": \"vi\",\n    \"ÁÆÄ‰Ωì‰∏≠Êñá\": \"zh\"\n}\n\ndef _get_language(code):\n    return languages[code]\n\nfinal_language = _get_language(language)\n\nwork_dir = '/kaggle/working'\noutput_path = os.path.join(work_dir, 'piper_output') \noutput_dir = os.path.join(output_path, model_name)\n\nos.makedirs(output_dir, exist_ok=True)\n\naudio_cache_dir = os.path.join(work_dir, 'audio_cache')\nos.makedirs(audio_cache_dir, exist_ok=True)\n\n# Resampling \nresample = False\nif resample:\n    input_wavs_dir = wavs_dir  \n    output_wavs_dir = '/kaggle/working/wavs_resampled'\n\n    os.makedirs(output_wavs_dir, exist_ok=True)\n    !python /kaggle/working/python/resample.py \\\n        --input_dir \"{input_wavs_dir}\" \\\n        --output_dir \"{output_wavs_dir}\" \\\n        --output_sr {sample_rate} \\\n        --file_ext \"wav\"\n\nif use_whisper:\n    print(\"Transcript file missing. Generating transcripts with Whisper...\")\n    make_dataset(f\"{wavs_dir}\", final_language[:2])\n    print(\"Transcription done!\")\n\nprint(\"Starting preprocessing...\")\n%cd /kaggle/working/python\n!python -m piper_train.preprocess \\\n  --language {final_language} \\\n  --input-dir \"{sample_path}\" \\\n  --cache-dir \"{audio_cache_dir}\" \\\n  --output-dir \"{output_dir}\" \\\n  --dataset-name \"{model_name}\" \\\n  --dataset-format {dataset_format} \\\n  --sample-rate {sample_rate} \\\n  {force_sp}\n\n#print(\"ls /kaggle/working/piper_output/epic\")\n#!ls /kaggle/working/piper_output/epic\n#print(\"ls /kaggle/working/audio_cache\")\n#!ls /kaggle/working/audio_cache\n#print(\"cat config\")\n#!cat /kaggle/working/piper_output/epic/config.json\nprint(\"Preprocessing done!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T13:30:51.668653Z","iopub.execute_input":"2025-06-08T13:30:51.668898Z","iopub.status.idle":"2025-06-08T13:30:54.941490Z","shell.execute_reply.started":"2025-06-08T13:30:51.668882Z","shell.execute_reply":"2025-06-08T13:30:54.940607Z"}},"outputs":[{"name":"stdout","text":"Starting preprocessing...\n/kaggle/working/python\nPreprocessing done!\n","output_type":"stream"}],"execution_count":59},{"cell_type":"code","source":"#@markdown # <font color=\"ffc800\"> **4. Settings.** üß∞\n#@markdown ---\nimport json\nfrom IPython.display import display\nimport os\nimport re\nimport glob\nimport json\nimport os\nimport gdown\n#@markdown ### <font color=\"orange\">**Select the action to train this dataset: (READ CAREFULLY)**\n\n#@markdown * The option to <font color=\"orange\">continue a training</font> is self-explanatory. If you've previously trained a model with free colab, your time is up and you're considering training it some more, this is ideal for you. You just have to set the same settings that you set when you first trained this model.\n#@markdown * The option to <font color=\"orange\">convert a single-speaker model to a multi-speaker model</font> is self-explanatory, and for this it is important that you have processed a dataset that contains text and audio from all possible speakers that you want to train in your model.\n#@markdown * The <font color=\"orange\">finetune</font> option is used to train a dataset using a pretrained model, that is, train on that data. This option is ideal if you want to train a very small dataset (more than five minutes recommended).\n#@markdown * The <font color=\"orange\">train from scratch</font> option builds features such as dictionary and speech form from scratch, and this may take longer to converge. For this, hours of audio (8 at least) are recommended, which have a large collection of phonemes.\n\naction = \"finetune\" #@param [\"Continue training\", \"convert single-speaker to multi-speaker model\", \"finetune\", \"train from scratch\"]\n#@markdown ---\nif action == \"Continue training\":\n    checkpoints = glob.glob(f\"{output_dir}/lightning_logs/**/checkpoints/last.ckpt\", recursive=True)\n    if len(checkpoints):\n        last_checkpoint = sorted(checkpoints, key=lambda x: int(re.findall(r'version_(\\d+)', x)[0]))[-1]\n        ft_command = f'--resume_from_checkpoint \"{last_checkpoint}\" '\n        print(f\"Continuing {model_name}'s training at: {last_checkpoint}\")\n    else:\n        raise Exception(\"Training cannot be continued as there is no checkpoint to continue at.\")\nelif action == \"finetune\":\n    if os.path.exists(f\"{output_dir}/lightning_logs/version_0/checkpoints/last.ckpt\"):\n        raise Exception(\"Oh no! You have already trained this model before, you cannot choose this option since your progress will be lost, and then your previous time will not count. Please select the option to continue a training.\")\n    else:\n        ft_command = f\"--resume_from_checkpoint /kaggle/working/pretrained/{checkpoint_name_pretrained_model} \"\nelif action == \"convert single-speaker to multi-speaker model\":\n    if not single_speaker:\n        ft_command = f\"--resume_from_single_speaker_checkpoint /kaggle/working/pretrained/{checkpoint_name_pretrained_model} \"\n    else:\n        raise Exception(\"This dataset is not a multi-speaker dataset!\")\nelse:\n    ft_command = \"\"\nif action == \"convert single-speaker to multi-speaker model\" or action == \"finetune\":\n    if os.path.exists(f\"/kaggle/working/pretrained/{checkpoint_name_pretrained_model}\"):\n        print(\"\\033[93mModel found!\")\n    else:\n        raise Exception(\"No Pretrained Model!\")\n        \n    #try:\n    #    with open('/kaggle/input/piper-orig/piper-master/notebooks/pretrained_models.json') as f:\n    #        pretrained_models = json.load(f)\n    #    \n    #    if final_language in pretrained_models:\n    #        models = pretrained_models[final_language]\n    #        model_names = list(models.keys())  # Liste der Modellnamen\n    #        print(f\"Verf√ºgbare Modelle: {model_names}\")\n\n\n            \n            # Anstatt ein Dropdown-Men√º zu verwenden, w√§hle ein Modell per Eingabe\n            #model_name = input(\"Bitte w√§hle ein Modell aus der Liste (z.B. 'epic'): \")\n            #if model_name in models:\n            #    model_url = models[model_name]\n            #    print(f\"\\033[93mModel {model_name} wird heruntergeladen...\")\n            #    if model_url.startswith(\"1\"):\n            #        gdown.download(model_url, \"/kaggle/working/pretrained.ckpt\", quiet=False)\n            #    elif model_url.startswith(\"https://drive.google.com/file/d/\"):\n            #        gdown.download(model_url, \"/kaggle/working/pretrained.ckpt\", quiet=False, fuzzy=True)\n            #    else:\n            #        os.system(f\"wget --quiet {model_url} -O /kaggle/working/pretrainted_model/pretrained.ckpt\")\n                \n            #    if os.path.exists(\"/kaggle/working/pretrained.ckpt\"):\n            #        print(\"\\033[93mModel heruntergeladen!\")\n            #    else:\n            #        raise Exception(\"Das Pretrained-Modell konnte nicht heruntergeladen werden!\")\n            #else:\n            #    raise Exception(f\"Das Modell {model_name} existiert nicht in der Liste.\")\n    #    else:\n    #        raise Exception(f\"Es gibt keine vortrainierten Modelle f√ºr die Sprache {final_language}\")\n    #except FileNotFoundError:\n    #    raise Exception(\"Die Datei pretrained_models.json wurde nicht gefunden.\")\n\nelse:\n    print(\"\\033[93mWarning: this model will be trained from scratch. You need at least 8 hours of data for everything to work decent. Good luck!\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T13:31:00.787334Z","iopub.execute_input":"2025-06-08T13:31:00.788127Z","iopub.status.idle":"2025-06-08T13:31:00.797178Z","shell.execute_reply.started":"2025-06-08T13:31:00.788097Z","shell.execute_reply":"2025-06-08T13:31:00.796407Z"}},"outputs":[{"name":"stdout","text":"\u001b[93mModel found!\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"#@markdown # <font color=\"ffc800\"> **6. Train.** üèãÔ∏è‚Äç‚ôÇÔ∏è\n#@markdown ---\n#@markdown ### Run this cell to train your final model!\n\n#@markdown ---\n#@markdown ### <font color=\"orange\">**Disable validation?**\n#@markdown By uncheck this checkbox, this will allow to train the full dataset, without using any audio files or examples as a validation set. So, it will not be able to generate audios on the tensorboard while it's training. It is recommended to disable validation on extremely small datasets.\n\nvalidation = True #@param {type:\"boolean\"}\nif validation:\n    validation_split = 0.01\n    num_test_examples = 1\nelse:\n    validation_split = 0\n    num_test_examples = 0\n\nif not save_last:\n    save_last_command = \"\"\nelse:\n    save_last_command = \"--save_last True \"\n%ls /kaggle/working/\n%cd /kaggle/working/python\n# Ensure the training command uses paths that exist in the Kaggle environment\nget_ipython().system(f'''\npython -m piper_train \\\n--dataset-dir {output_dir} \\\n--accelerator 'gpu' \\\n--devices 1 \\\n--batch-size {batch_size} \\\n--validation-split {validation_split} \\\n--num-test-examples {num_test_examples} \\\n--quality {quality} \\\n--checkpoint-epochs {checkpoint_epochs} \\\n--num_ckpt {num_ckpt} \\\n{save_last_command}\\\n--log_every_n_steps {log_every_n_steps} \\\n--max_epochs {max_epochs} \\\n{ft_command}\\\n--precision {precision}\n''')\n\nprint(\"---------------------------------------------------------------------------------------------------------------------------------------\")\nprint(f\"training finished! saved to {output_dir}\")\n!ls /kaggle/working/piper_output/epic","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T12:46:11.795121Z","iopub.status.idle":"2025-06-08T12:46:11.795451Z","shell.execute_reply.started":"2025-06-08T12:46:11.795272Z","shell.execute_reply":"2025-06-08T12:46:11.795290Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#when training is finished model needs to be exported to onnx\n#model files are saved in the specified working folder\n%ls /kaggle/working/piper_output/epic/lightning_logs/\n%ls /kaggle/working/piper_output/epic/lightning_logs/version_0/checkpoints","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T12:46:11.796216Z","iopub.status.idle":"2025-06-08T12:46:11.796509Z","shell.execute_reply.started":"2025-06-08T12:46:11.796334Z","shell.execute_reply":"2025-06-08T12:46:11.796344Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport subprocess\n\nprint(\"export ckpt + json file from training to ONNX...\")\n\n%ls /kaggle/working/\n%cd /kaggle/working/python\n# Eingaben:\n#model_url = \"https://...\"  # Direktlink zum .ckpt-Modell\n#config_url = \"https://...\"  # Direktlink zur .json-Konfig\n\nlanguage = \"de_DE\"\nvoice_name = \"epic\"\nquality = \"medium\"\nstreaming = False\n\noutput_path = \"/kaggle/working/piper_output/epic\"  # vom Training erzeugt\nckpt_path = os.path.join(output_path, \"lightning_logs/version_0/checkpoints\")\n#ckpt_path = os.path.join(output_path, \"lightning_logs/version_3/checkpoints\")\nmodel_path = os.path.join(ckpt_path, \"last.ckpt\")  # oder √§ndere, wenn du epoch=xxx.ckpt willst\nconfig_path = os.path.join(output_path, \"config.json\")\n\nexport_voice_name = f\"{language}-{voice_name}+RT-{quality}\" if streaming else f\"{language}-{voice_name}-{quality}\"\nexport_voice_path = f\"/kaggle/working/voice-{export_voice_name}\"\npackages_path = \"/kaggle/working/packages\"\n#model_path = \"/kaggle/working/model.ckpt\"\n#config_path = f\"{export_voice_path}/{export_voice_name}.onnx.json\"\n\nos.makedirs(export_voice_path, exist_ok=True)\nos.makedirs(packages_path, exist_ok=True)\n\n#subprocess.run([\"wget\", model_url, \"-O\", model_path])\n#subprocess.run([\"wget\", config_url, \"-O\", config_path])\n\nconfig_export_path = os.path.join(export_voice_path, f\"{export_voice_name}.onnx.json\")\nwith open(config_path, \"r\", encoding=\"utf-8\") as f:\n    config = json.load(f)\n\nif streaming:\n    config[\"streaming\"] = True\n    config[\"key\"] = export_voice_name\n\nwith open(config_export_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump(config, f, indent=4)\n\n# Exportieren\nif streaming:\n    subprocess.run([\n        \"python\", \"-m\", \"piper_train.export_onnx_streaming\",\n        model_path, export_voice_path\n    ])\nelse:\n    subprocess.run([\n        \"python\", \"-m\", \"piper_train.export_onnx\",\n        model_path, f\"{export_voice_path}/{export_voice_name}.onnx\"\n    ])\n\n\n# Paket erstellen\nsubprocess.run([\n    \"tar\", \"-czvf\",\n    f\"{packages_path}/{export_voice_name}.tar.gz\",\n    \"-C\", export_voice_path, \".\"\n])\n\nprint(f\"finshied: file = {packages_path}/{export_voice_name}.tar.gz\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T12:46:11.797560Z","iopub.status.idle":"2025-06-08T12:46:11.797831Z","shell.execute_reply.started":"2025-06-08T12:46:11.797680Z","shell.execute_reply":"2025-06-08T12:46:11.797693Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if not os.path.exists(\"/kaggle/input/piper-tts\"):\n    raise Exception(\"upload piper tts binary piper_arch.tar.gz (e.g. piper_amd64.tar.gz) as piper-tts first\")\n!cp -r /kaggle/input/piper-tts /kaggle/working/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T12:46:11.799028Z","iopub.status.idle":"2025-06-08T12:46:11.799335Z","shell.execute_reply.started":"2025-06-08T12:46:11.799176Z","shell.execute_reply":"2025-06-08T12:46:11.799190Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import subprocess\nimport os\n\nmodel_path = f\"{export_voice_path}/{export_voice_name}.onnx\"\n\ntext = \"this is a test!\"\n\nwork_dir = '/kaggle/working'\noutput_dir = os.path.join(work_dir, 'tts-test')  \nos.makedirs(output_dir, exist_ok=True)\noutput_file = os.path.join(output_dir, \"test.wav\")\n\n# Schritt 5: Weitere Einstellungen\ndataset_format = \"ljspeech\"  # Formattyp\nsingle_speaker = True\nforce_sp = \" --single-speaker\" if single_speaker else \"\"\nsample_rate = \"16000\"\n\n# Audio-Cache erstellen\naudio_cache_dir = os.path.join(work_dir, 'audio_cache')\nos.makedirs(audio_cache_dir, exist_ok=True)\n\n# Shell-Befehl mit Text √ºber Pipe weitergeben\nsubprocess.run(\n    f\"echo '{text}' | /kaggle/working/piper-tts/piper --model {model_path} --output_file {output_file}\",\n    shell=True,\n    check=True\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T12:46:11.800491Z","iopub.status.idle":"2025-06-08T12:46:11.800813Z","shell.execute_reply.started":"2025-06-08T12:46:11.800636Z","shell.execute_reply":"2025-06-08T12:46:11.800652Z"}},"outputs":[],"execution_count":null}]}